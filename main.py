import os
import time                # ç”¨äºç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å
import pickle
import warnings
import random
import copy
import numpy as np
import pandas as pd        # [å…³é”®ä¿®å¤] è§£å†³ NameError: name 'pd' is not defined
from collections import Counter # [å…³é”®] åé¢èšåˆä»£ç ç”¨åˆ°äº† Counter
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import HeteroData
from torch_geometric.nn import HGTConv
from tqdm import tqdm
from sklearn.metrics import (
    roc_auc_score, average_precision_score, accuracy_score,
    f1_score, precision_score, recall_score, matthews_corrcoef
)

warnings.filterwarnings("ignore")

# ==========================================
# 1. å…¨å±€é…ç½®
# ==========================================
DATA_PKL = "data_reified.pkl"
MODEL_SAVE_PATH = "best_model_final.pth"
SEEDS = [0, 7, 100, 3407, 2000]
TOP_K_PRED = 50 
# è®­ç»ƒè¶…å‚æ•°
NUM_EPOCHS = 100
BATCH_SIZE = 1024
LEARNING_RATE = 1e-3
WEIGHT_DECAY = 1e-5
EMBED_DIM = 128
NUM_HEADS = 4

# [æ–°å¢] åŸå‹é…ç½® (æ›¿ä»£åŸæ¥çš„ NUM_PROTOTYPES)
PROTO_CONFIG = {
    'pesticide': 10,
    'disease': 10,
    'plant': 10
}

# è®¾å¤‡é…ç½® (å¿…é¡»åœ¨æ¨¡å‹å®šä¹‰å‰å®šä¹‰ï¼Œå› ä¸ºæ–°æ¨¡å‹ä»£ç ä¸­å¼•ç”¨äº†å…¨å±€ DEVICE)
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"-> Using device: {DEVICE}")

# ==========================================
# 2. åŸºç¡€ç»„ä»¶ (Model Components) - [å·²æ›¿æ¢]
# ==========================================

class AttentionFusion(nn.Module):
    """
    å¤šå°ºåº¦æ³¨æ„åŠ›èåˆæ¨¡å—ï¼š
    å¯¹ HGT ä¸åŒå±‚ (Layer 1, 2, 3) çš„è¾“å‡ºè¿›è¡Œ Self-Attention åŠ æƒèåˆã€‚
    """
    def __init__(self, embed_dim=128, num_heads=4):
        super().__init__()
        self.attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)
        self.layer_norm = nn.LayerNorm(embed_dim)

    def forward(self, inputs):
        # inputs: list of tensors [x1, x2, x3], each shape (Batch, Dim)
        stacked = torch.stack(inputs, dim=1)  # (Batch, Num_Layers, Dim)
        attn_output, _ = self.attention(stacked, stacked, stacked)
        fused = attn_output.mean(dim=1)       # (Batch, Dim)
        return self.layer_norm(fused)

class HGTBlock(nn.Module):
    """
    HGT ç¼–ç å—ï¼šåŒ…å« HGTConv, LayerNorm, ReLU å’Œ Dropoutã€‚
    """
    def __init__(self, in_channels, out_channels, metadata, heads=4, dropout=0.2):
        super().__init__()
        # PyG HGTConv é»˜è®¤ group='sum'
        self.hgt = HGTConv(in_channels, out_channels, metadata, heads)
        self.norm = nn.LayerNorm(out_channels)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x_dict, edge_dict, res_dict=None):
        x_new = self.hgt(x_dict, edge_dict)
        x_out = {}
        for k in x_new:
            x = x_new[k]
            # Residual Connection
            if res_dict is not None:
                res = res_dict.get(k, None)
                if res is not None and res.shape[-1] == x.shape[-1]:
                    x = x + res
            x = self.norm(x)
            x = F.relu(x)
            x = self.dropout(x)
            x_out[k] = x
        return x_out

class PrototypeRefiner(nn.Module):
    """
    åŸå‹ç²¾ç‚¼æ¨¡å— (å¸¦é—¨æ§èåˆ)ï¼š
    1. å­¦ä¹ ä¸€ç»„å¯è®­ç»ƒçš„åŸå‹å‘é‡ (Prototypes)ã€‚
    2. è®¡ç®—èŠ‚ç‚¹ä¸åŸå‹çš„ç›¸ä¼¼åº¦ï¼Œé‡æ„å‡ºâ€œç†æƒ³åŒ–ç‰¹å¾â€ã€‚
    3. ä½¿ç”¨é—¨æ§æœºåˆ¶ (Gate) å°†ç†æƒ³åŒ–ç‰¹å¾ä¸åŸå§‹ç‰¹å¾èåˆã€‚
    """
    def __init__(self, num_prototypes, embed_dim, k=3, temperature=0.1):
        super().__init__()
        self.prototypes = nn.Parameter(torch.empty(num_prototypes, embed_dim))
        nn.init.orthogonal_(self.prototypes)
        
        # å˜æ¢åŸå‹ç‰¹å¾çš„ MLP
        self.proto_transform = nn.Sequential(
            nn.Linear(embed_dim, embed_dim), 
            nn.ReLU(), 
            nn.LayerNorm(embed_dim)
        )
        
        # [æ ¸å¿ƒ] é—¨æ§ç½‘ç»œï¼šè¾“å…¥ (åŸå§‹ç‰¹å¾ + åŸå‹ç‰¹å¾)ï¼Œè¾“å‡ºé—¨æ§ç³»æ•° (0~1)
        self.gate_net = nn.Sequential(
            nn.Linear(embed_dim * 2, embed_dim),
            nn.Sigmoid()
        )
        
        self.final_norm = nn.LayerNorm(embed_dim)
        self.k = min(k, num_prototypes)
        self.warmup = True 
        self.temperature = temperature

    def set_warmup(self, status: bool):
        self.warmup = status

    def get_regularization_loss(self, batch_mean_probs):
        """
        è®¡ç®—æ­£åˆ™åŒ–æŸå¤±ï¼š
        1. æ­£äº¤æŸå¤± (Orthogonal Loss): ä¿è¯åŸå‹å¤šæ ·æ€§
        2. å‡è¡¡æŸå¤± (Balance Loss): ä¿è¯åŸå‹åˆ©ç”¨ç‡å‡è¡¡
        """
        p_norm = F.normalize(self.prototypes, dim=1)
        sim_matrix = p_norm @ p_norm.T
        identity = torch.eye(sim_matrix.size(0), device=sim_matrix.device)
        ortho_loss = F.mse_loss(sim_matrix, identity)
        
        target_prob = 1.0 / self.prototypes.size(0)
        balance_loss = F.mse_loss(batch_mean_probs, torch.full_like(batch_mean_probs, target_prob))
        return ortho_loss + 2.0 * balance_loss

    def forward(self, x_instance):
        # 1. ç›¸ä¼¼åº¦åŒ¹é…
        x_norm = F.normalize(x_instance, dim=1)
        p_norm = F.normalize(self.prototypes, dim=1)
        logits = (x_norm @ p_norm.T) / self.temperature
        probs = torch.softmax(logits, dim=1)
        batch_mean_probs = probs.mean(dim=0) 
        
        # 2. Top-K ç¨€ç–åŒ–
        if not self.warmup and self.k < logits.size(1):
            topk_values, topk_indices = torch.topk(logits, self.k, dim=1)
            mask = torch.full_like(logits, float('-inf'))
            mask.scatter_(1, topk_indices, topk_values)
            sim = torch.softmax(mask, dim=1)
        else:
            sim = probs 
            
        # 3. ç‰¹å¾é‡æ„ (Abstraction)
        x_abstract = sim @ self.prototypes
        
        # 4. é—¨æ§èåˆ (Gated Fusion)
        x_proto_transformed = self.proto_transform(x_abstract)
        
        # è®¡ç®—é—¨æ§ç³»æ•°
        concat_feat = torch.cat([x_instance, x_proto_transformed], dim=1)
        gate = self.gate_net(concat_feat)
        
        # èåˆï¼šåŸå§‹ç‰¹å¾ + (é—¨æ§ * åŸå‹ä¿®æ­£é‡)
        x_fused = x_instance + gate * x_proto_transformed
        
        # 5. æœ€ç»ˆå½’ä¸€åŒ–
        x_final = self.final_norm(x_fused)
        
        return x_final, sim, batch_mean_probs

class TriplePredictor(nn.Module):
    """
    ä¸‰å…ƒç»„é¢„æµ‹å¤´ï¼šè¾“å…¥ (P, D, Pl) -> è¾“å‡º Logits
    """
    def __init__(self, in_channels, hidden_channels=128):
        super().__init__()
        self.lin1 = nn.Linear(in_channels * 3, hidden_channels)
        self.batch_norm1 = nn.BatchNorm1d(hidden_channels)
        self.lin2 = nn.Linear(hidden_channels, 64)
        self.batch_norm2 = nn.BatchNorm1d(64)
        self.lin3 = nn.Linear(64, 1)
        self.dropout = nn.Dropout(0.3)

    def forward(self, p_emb, d_emb, pl_emb):
        x = torch.cat([p_emb, d_emb, pl_emb], dim=1)
        x = F.relu(self.batch_norm1(self.lin1(x)))
        x = self.dropout(x)
        x = F.relu(self.batch_norm2(self.lin2(x)))
        x = self.dropout(x)
        return self.lin3(x).squeeze()

# ==========================================
# 3. ä¸»æ¨¡å‹ (Full Model) - [å·²æ›¿æ¢]
# ==========================================
class MultiModelNetV2(nn.Module):
    # [ä¿®æ”¹] num_prototypes æ¥æ”¶å­—å…¸ï¼Œä¾‹å¦‚ {'pesticide': 20, 'disease': 15, 'plant': 5}
    def __init__(self, metadata, input_channels_dict, num_events, embed_dim=128, heads=4, 
                 proto_config={'pesticide': 5, 'disease': 5, 'plant': 5}):
        super().__init__()
        
        # 1. Input Projection (ç‰¹å¾å¯¹é½)
        self.input_projs = nn.ModuleDict()
        for node_type, in_dim in input_channels_dict.items():
            if node_type != 'event':
                self.input_projs[node_type] = nn.Linear(in_dim, embed_dim)
        
        # Event èŠ‚ç‚¹ä½¿ç”¨ Embedding (å¯å­¦ä¹  ID)
        self.event_emb = nn.Embedding(num_events, embed_dim)

        # 2. HGT Encoder (3 Layers)
        self.block1 = HGTBlock(embed_dim, embed_dim, metadata, heads)
        self.block2 = HGTBlock(embed_dim, embed_dim, metadata, heads)
        self.block3 = HGTBlock(embed_dim, embed_dim, metadata, heads) 

        # 3. Multi-Scale Fusion
        self.fusion_p = AttentionFusion(embed_dim, heads)
        self.fusion_d = AttentionFusion(embed_dim, heads)
        self.fusion_pl = AttentionFusion(embed_dim, heads)

        # 4. Prototype Refiner (Gated Version)
        # [å…³é”®ä¿®æ”¹] åˆ†åˆ«è¯»å–é…ç½®ï¼Œé’ˆå¯¹æ€§åˆå§‹åŒ–
        # ä½¿ç”¨ .get æä¾›é»˜è®¤å€¼ï¼Œé˜²æ­¢æŠ¥é”™
        num_p = proto_config.get('pesticide', 10)
        num_d = proto_config.get('disease', 10)
        num_pl = proto_config.get('plant', 10)

        print(f"Initializing Prototypes: Pesticide={num_p}, Disease={num_d}, Plant={num_pl}")

        self.refiner_p = PrototypeRefiner(num_prototypes=num_p, embed_dim=embed_dim, k=min(3, num_p))
        self.refiner_d = PrototypeRefiner(num_prototypes=num_d, embed_dim=embed_dim, k=min(3, num_d))
        self.refiner_pl = PrototypeRefiner(num_prototypes=num_pl, embed_dim=embed_dim, k=min(3, num_pl))
        
        # 5. Predictor
        self.predictor = TriplePredictor(embed_dim)
        
        # ç”¨äºå­˜å‚¨ Refiner çš„æ¦‚ç‡åˆ†å¸ƒä»¥è®¡ç®— Loss
        self.last_probs = {'p': None, 'd': None, 'pl': None}

    def set_warmup(self, status):
        """æ§åˆ¶ Refiner æ˜¯å¦å¼€å¯ Top-K (Warmup æœŸé—´å…³é—­)"""
        self.refiner_p.set_warmup(status)
        self.refiner_d.set_warmup(status)
        self.refiner_pl.set_warmup(status)

    def get_proto_reg_loss(self):
        """è·å–æ‰€æœ‰ Refiner çš„æ­£åˆ™åŒ–æŸå¤±ä¹‹å’Œ"""
        loss = torch.tensor(0.0, device=DEVICE) # ä½¿ç”¨å…¨å±€ DEVICE
        for key in self.last_probs:
            if self.last_probs[key] is not None:
                refiner = getattr(self, f"refiner_{key}")
                loss += refiner.get_regularization_loss(self.last_probs[key])
        return loss

    def forward(self, x_dict, edge_index_dict):
        # A. Input Embedding
        x_emb = {}
        for ntype, x in x_dict.items():
            if ntype == 'event':
                event_ids = torch.arange(x.shape[0], device=x.device)
                x_emb[ntype] = self.event_emb(event_ids)
            elif ntype in self.input_projs:
                x_emb[ntype] = F.relu(self.input_projs[ntype](x))
        
        # B. HGT Layers (3-hop propagation)
        x1 = self.block1(x_emb, edge_index_dict, x_emb)
        x2 = self.block2(x1, edge_index_dict, x1)
        x3 = self.block3(x2, edge_index_dict, x2)
        
        # C. Fusion & Refinement
        
        # Pesticide Branch
        p_raw = self.fusion_p([x1['pesticide'], x2['pesticide'], x3['pesticide']])
        p_final, _, p_probs = self.refiner_p(p_raw)
        self.last_probs['p'] = p_probs
        
        # Disease Branch
        d_raw = self.fusion_d([x1['disease'], x2['disease'], x3['disease']])
        d_final, _, d_probs = self.refiner_d(d_raw)
        self.last_probs['d'] = d_probs
        
        # Plant Branch
        pl_raw = self.fusion_pl([x1['plant'], x2['plant'], x3['plant']])
        pl_final, _, pl_probs = self.refiner_pl(pl_raw)
        self.last_probs['pl'] = pl_probs
        
        return {'pesticide': p_final, 'disease': d_final, 'plant': pl_final}

    def predict_triplets(self, p_emb, d_emb, pl_emb):
        return self.predictor(p_emb, d_emb, pl_emb)

# ==========================================
# 4. å·¥å…·å‡½æ•°
# ==========================================
def seed_everything(seed):
    print(f"-> Setting global seed to: {seed}")
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def calculate_metrics(y_true, y_probs, threshold=0.5):
    """è®¡ç®—å…¨å¥—è¯„ä»·æŒ‡æ ‡"""
    y_pred = (y_probs > threshold).astype(int)
    return {
        'AUC': roc_auc_score(y_true, y_probs),
        'AP': average_precision_score(y_true, y_probs),
        'Acc': accuracy_score(y_true, y_pred),
        'F1': f1_score(y_true, y_pred),
        'Precision': precision_score(y_true, y_pred),
        'Recall': recall_score(y_true, y_pred),
        'MCC': matthews_corrcoef(y_true, y_pred)
    }

def get_triplets_from_events(data, event_mask=None):
    """ä» Event èŠ‚ç‚¹æå– (P, D, Pl) ä¸‰å…ƒç»„"""
    pe_edges = data['pesticide', 'participates_in', 'event'].edge_index
    de_edges = data['disease', 'is_target_of', 'event'].edge_index
    ple_edges = data['plant', 'is_host_of', 'event'].edge_index
    
    e_to_p = dict(zip(pe_edges[1].tolist(), pe_edges[0].tolist()))
    e_to_d = dict(zip(de_edges[1].tolist(), de_edges[0].tolist()))
    e_to_pl = dict(zip(ple_edges[1].tolist(), ple_edges[0].tolist()))
    
    triplets = []
    all_event_indices = torch.arange(data['event'].num_nodes)
    target_events = all_event_indices[event_mask] if event_mask is not None else all_event_indices

    for e_idx in target_events.tolist():
        if e_idx in e_to_p and e_idx in e_to_d and e_idx in e_to_pl:
            triplets.append([e_to_p[e_idx], e_to_d[e_idx], e_to_pl[e_idx]])
            
    return torch.tensor(triplets, dtype=torch.long)

def sample_negative_triplets(pos_triplets, num_nodes_dict):
    """1:1 è´Ÿé‡‡æ ·"""
    num_pos = len(pos_triplets)
    # å¹³å‡åˆ†é…è´Ÿé‡‡æ ·ç­–ç•¥ï¼š1/3 æ›¿æ¢Pï¼Œ1/3 æ›¿æ¢Dï¼Œ1/3 æ›¿æ¢Pl
    num_neg_p = num_pos // 3
    num_neg_d = num_pos // 3
    num_neg_pl = num_pos - num_neg_p - num_neg_d

    def _sample(triplets, col_idx, max_idx):
        neg = triplets.clone()
        neg[:, col_idx] = torch.randint(0, max_idx, (len(triplets),))
        return neg

    neg_p = _sample(pos_triplets[:num_neg_p], 0, num_nodes_dict['p'])
    neg_d = _sample(pos_triplets[num_neg_p:num_neg_p+num_neg_d], 1, num_nodes_dict['d'])
    neg_pl = _sample(pos_triplets[num_neg_p+num_neg_d:], 2, num_nodes_dict['pl'])

    return torch.cat([neg_p, neg_d, neg_pl], dim=0)

def mask_graph_by_events(data_orig, event_mask, device):
    """
    æ ¹æ® event_mask åŠ¨æ€æ„å»ºå›¾ç»“æ„ã€‚
    è®­ç»ƒæ—¶åªä¿ç•™ Train Event è¾¹ï¼Œé˜²æ­¢ Val/Test ä¿¡æ¯æ³„éœ²ã€‚
    """
    data_masked = data_orig.clone()
    valid_event_indices = torch.where(event_mask.to(device))[0]
    
    for edge_type in data_masked.edge_types:
        src_type, _, dst_type = edge_type
        edge_index = data_masked[edge_type].edge_index.to(device)
        mask = None
        if dst_type == 'event':
            mask = torch.isin(edge_index[1], valid_event_indices)
        elif src_type == 'event':
            mask = torch.isin(edge_index[0], valid_event_indices)
            
        if mask is not None:
            data_masked[edge_type].edge_index = edge_index[:, mask]
            
    return data_masked

# ==========================================
# 5. ä¸»è®­ç»ƒæµç¨‹
# ==========================================
# def main():
def train_model(seed, data_full):
    """
    è®­ç»ƒå•ä¸ªç§å­çš„æ¨¡å‹ï¼Œå¹¶è¿”å›ä¿å­˜è·¯å¾„
    """
    seed_everything(seed)
    
    # [ä¿®æ”¹] åŠ¨æ€æ–‡ä»¶åï¼Œé˜²æ­¢è¦†ç›–
    save_path = f"best_model_seed_{seed}.pth"
    
    # 2. è·å–èŠ‚ç‚¹ç»Ÿè®¡
    num_pesticides = data_full['pesticide'].num_nodes
    num_diseases = data_full['disease'].num_nodes
    num_plants = data_full['plant'].num_nodes
    num_events = data_full['event'].num_nodes
    num_nodes_dict = {'p': num_pesticides, 'd': num_diseases, 'pl': num_plants}
    
    # 3. æ•°æ®åˆ’åˆ† (80/10/10)
    indices = torch.randperm(num_events)
    split1 = int(num_events * 0.8)
    split2 = int(num_events * 0.9)
    
    train_mask = torch.zeros(num_events, dtype=torch.bool); train_mask[indices[:split1]] = True
    val_mask = torch.zeros(num_events, dtype=torch.bool); val_mask[indices[split1:split2]] = True
    # test_mask = torch.zeros(num_events, dtype=torch.bool); test_mask[indices[split2:]] = True 
    # åœ¨é›†æˆæµç¨‹ä¸­ï¼Œè¿™é‡Œå¯ä»¥ç•¥è¿‡ testï¼Œæˆ–è€…ä»…ä»…æ‰“å°ä¸€ä¸‹ verify æ€§èƒ½
    
    train_triplets = get_triplets_from_events(data_full, train_mask)
    val_triplets = get_triplets_from_events(data_full, val_mask)
    
    # 4. åˆå§‹åŒ–æ¨¡å‹
    input_channels = {nt: data_full[nt].x.shape[1] for nt in data_full.node_types}
    
    model = MultiModelNetV2(
        metadata=data_full.metadata(),
        input_channels_dict=input_channels,
        num_events=num_events,
        embed_dim=EMBED_DIM,
        heads=NUM_HEADS,
        proto_config=PROTO_CONFIG
    ).to(DEVICE)
    
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
    criterion = nn.BCEWithLogitsLoss()

    best_val_auc = 0.0
    patience = 10
    no_improve_cnt = 0
    
    # 5. è®­ç»ƒå¾ªç¯ (ä½¿ç”¨ tqdm)
    # leave=False é¿å…å¤šè½®è®­ç»ƒåˆ·å±
    pbar = tqdm(range(NUM_EPOCHS), desc=f"Training Seed {seed}", leave=False)
    
    for epoch in pbar:
        model.train()
        model.set_warmup(epoch < 10)
        
        perm = torch.randperm(train_triplets.size(0))
        triplets_shuffled = train_triplets[perm]
        
        total_loss = 0
        num_batches = (len(triplets_shuffled) + BATCH_SIZE - 1) // BATCH_SIZE
        
        train_graph = mask_graph_by_events(data_full, train_mask, DEVICE).to(DEVICE)
        
        for i in range(num_batches):
            optimizer.zero_grad()
            out_emb = model(train_graph.x_dict, train_graph.edge_index_dict)
            
            batch_pos = triplets_shuffled[i*BATCH_SIZE : (i+1)*BATCH_SIZE]
            batch_neg = sample_negative_triplets(batch_pos, num_nodes_dict)
            
            batch_all = torch.cat([batch_pos, batch_neg], dim=0).to(DEVICE)
            labels = torch.cat([torch.ones(len(batch_pos)), torch.zeros(len(batch_neg))]).to(DEVICE)
            
            logits = model.predict_triplets(
                out_emb['pesticide'][batch_all[:, 0]],
                out_emb['disease'][batch_all[:, 1]],
                out_emb['plant'][batch_all[:, 2]]
            )
            
            loss = criterion(logits, labels)
            if epoch >= 10:
                loss += 0.1 * model.get_proto_reg_loss()
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            total_loss += loss.item()
            
        # éªŒè¯é˜¶æ®µ
        if (epoch + 1) % 5 == 0:
            model.eval()
            with torch.no_grad():
                val_graph = mask_graph_by_events(data_full, train_mask, DEVICE).to(DEVICE)
                out_val = model(val_graph.x_dict, val_graph.edge_index_dict)
                
                v_neg = sample_negative_triplets(val_triplets, num_nodes_dict)
                v_all = torch.cat([val_triplets, v_neg], dim=0).to(DEVICE)
                v_lbl = torch.cat([torch.ones(len(val_triplets)), torch.zeros(len(v_neg))]).cpu().numpy()
                
                v_logits = model.predict_triplets(
                    out_val['pesticide'][v_all[:, 0]],
                    out_val['disease'][v_all[:, 1]],
                    out_val['plant'][v_all[:, 2]]
                )
                v_probs = torch.sigmoid(v_logits).cpu().numpy()
                val_auc = roc_auc_score(v_lbl, v_probs)

                if val_auc > best_val_auc:
                    best_val_auc = val_auc
                    torch.save(model.state_dict(), save_path)
                    no_improve_cnt = 0
                else:
                    no_improve_cnt += 1
            
            pbar.set_postfix({'Loss': total_loss/num_batches, 'Best Val AUC': best_val_auc})
            
            if no_improve_cnt >= patience:
                pbar.close()
                break # æ—©åœ

    return save_path
def predict_new_links(current_seed, model_path, data_full):
    print(f"Generating predictions for Seed: {current_seed}...")
    seed_everything(current_seed)

    # å¿…è¦çš„ç»Ÿè®¡æ•°æ®
    num_pesticides = data_full['pesticide'].num_nodes
    num_events = data_full['event'].num_nodes

    input_channels = {nt: data_full[nt].x.shape[1] for nt in data_full.node_types}
    model = MultiModelNetV2(
        metadata=data_full.metadata(),
        input_channels_dict=input_channels,
        num_events=num_events,
        embed_dim=EMBED_DIM,
        heads=NUM_HEADS,
        proto_config=PROTO_CONFIG 
    ).to(DEVICE)

    if not os.path.exists(model_path):
        print(f"Error: Model file {model_path} not found.")
        return None
        
    model.load_state_dict(torch.load(model_path, map_location=DEVICE))
    model.eval()
    
    # é¢„æµ‹æ—¶ä½¿ç”¨å…¨å›¾ä¿¡æ¯
    full_graph = data_full.to(DEVICE)
    with torch.no_grad():
        out_emb_dict = model(full_graph.x_dict, full_graph.edge_index_dict)
    
    p_emb_all = out_emb_dict['pesticide']
    d_emb_all = out_emb_dict['disease']
    pl_emb_all = out_emb_dict['plant']
    
    # æ„é€ é¢„æµ‹å€™é€‰é›† (ä»…é¢„æµ‹å·²çŸ¥çš„ Disease-Plant ç»„åˆ)
    de_edges_np = data_full['disease', 'is_target_of', 'event'].edge_index.cpu().numpy()
    ple_edges_np = data_full['plant', 'is_host_of', 'event'].edge_index.cpu().numpy()
    
    e_to_d_map = dict(zip(de_edges_np[1], de_edges_np[0]))
    e_to_pl_map = dict(zip(ple_edges_np[1], ple_edges_np[0]))
    
    unique_d_pl_pairs = set()
    for e_idx in range(num_events):
        if e_idx in e_to_d_map and e_idx in e_to_pl_map:
            unique_d_pl_pairs.add((e_to_d_map[e_idx], e_to_pl_map[e_idx]))
            
    results = []
    # ä½¿ç”¨ batch å¤„ç†å†œè¯ï¼Œé¿å…å†…å­˜æº¢å‡º
    # å°† unique_d_pl_pairs è½¬ä¸º list ä»¥ä¾¿ç´¢å¼•
    pairs_list = list(unique_d_pl_pairs)
    
    for d_idx, pl_idx in tqdm(pairs_list, desc=f"[Seed {current_seed}] Predicting", leave=False):
        # ä¼˜åŒ–ï¼šä¸repeatï¼Œç›´æ¥åˆ©ç”¨ broadcasting æˆ–è€…åˆ†æ‰¹
        # è¿™é‡Œä¸ºäº†ç¨³å¦¥ï¼Œæ²¿ç”¨ä½ çš„é€»è¾‘ï¼Œä½†è¦æ³¨æ„ tensor ç»´åº¦
        d_emb = d_emb_all[d_idx].unsqueeze(0) # (1, Dim)
        pl_emb = pl_emb_all[pl_idx].unsqueeze(0) # (1, Dim)
        
        # æ‰©å±•åˆ°æ‰€æœ‰å†œè¯
        d_emb_batch = d_emb.expand(num_pesticides, -1)
        pl_emb_batch = pl_emb.expand(num_pesticides, -1)
        
        with torch.no_grad():
            logits = model.predict_triplets(p_emb_all, d_emb_batch, pl_emb_batch)
            scores = torch.sigmoid(logits)
            
        # Top-K
        topk_scores, topk_p_indices = torch.topk(scores, k=TOP_K_PRED)
        
        for score, p_idx in zip(topk_scores.cpu().tolist(), topk_p_indices.cpu().tolist()):
            results.append({
                'd_idx': d_idx,
                'pl_idx': pl_idx,
                'p_idx': p_idx,
                'score': score,
                'seed': current_seed
            })

    return pd.DataFrame(results)

# def aggregate_and_save_results(all_results_dfs, data_full):
#     print(f"\n{'='*60}")
#     print("ğŸš€ å¼€å§‹èšåˆå¤šç§å­é¢„æµ‹ç»“æœ (Ensemble Aggregation)")
#     print(f"{'='*60}\n")

#     if not all_results_dfs:
#         print("æ²¡æœ‰æ”¶é›†åˆ°é¢„æµ‹ç»“æœã€‚")
#         return

#     full_df = pd.concat(all_results_dfs, ignore_index=True)

#     agg_df = full_df.groupby(['d_idx', 'pl_idx', 'p_idx']).agg(
#         mean_score=('score', 'mean'),
#         std_score=('score', 'std'),
#         count=('seed', 'count')
#     ).reset_index()
    
#     # è¿‡æ»¤ä½å…±è¯†åº¦
#     MIN_VOTES = max(3, int(len(SEEDS) * 0.8)) # åŠ¨æ€è®¾å®šï¼šæ¯”å¦‚ 5ä¸ªç§å­è‡³å°‘è¦4ç¥¨
#     print(f"è¿‡æ»¤ä½å…±è¯†ç»“æœ (ä¿ç•™å¾—ç¥¨æ•° >= {MIN_VOTES})...")
#     agg_df = agg_df[agg_df['count'] >= MIN_VOTES]
    
#     # æ˜ å°„åç§°
#     p_names = data_full['pesticide'].names if hasattr(data_full['pesticide'], 'names') else [f"P_{i}" for i in range(data_full['pesticide'].num_nodes)]
#     d_names = data_full['disease'].names if hasattr(data_full['disease'], 'names') else [f"D_{i}" for i in range(data_full['disease'].num_nodes)]
#     pl_names = data_full['plant'].names if hasattr(data_full['plant'], 'names') else [f"Pl_{i}" for i in range(data_full['plant'].num_nodes)]

#     # æ„å»ºå·²çŸ¥ä¸‰å…ƒç»„æ£€æŸ¥
#     pe_edges = data_full['pesticide', 'participates_in', 'event'].edge_index.cpu()
#     de_edges = data_full['disease', 'is_target_of', 'event'].edge_index.cpu()
#     ple_edges = data_full['plant', 'is_host_of', 'event'].edge_index.cpu()
    
#     # å»ºç«‹æ˜ å°„ä»¥å¿«é€ŸæŸ¥æ‰¾ event å¯¹åº”çš„ P, D, Pl
#     event_p = dict(zip(pe_edges[1].tolist(), pe_edges[0].tolist()))
#     event_d = dict(zip(de_edges[1].tolist(), de_edges[0].tolist()))
#     event_pl = dict(zip(ple_edges[1].tolist(), ple_edges[0].tolist()))
    
#     known_triplets = set()
#     for e in range(data_full['event'].num_nodes):
#         if e in event_p and e in event_d and e in event_pl:
#             known_triplets.add((event_p[e], event_d[e], event_pl[e]))

#     # è®¡ç®—å†œè¯æµè¡Œåº¦ (ç”¨äºæƒ©ç½š)
#     p_popularity = Counter(pe_edges[0].tolist())

#     final_output = []
#     for _, row in agg_df.iterrows():
#         p_idx, d_idx, pl_idx = int(row['p_idx']), int(row['d_idx']), int(row['pl_idx'])
        
#         is_known = (p_idx, d_idx, pl_idx) in known_triplets
#         pop = p_popularity.get(p_idx, 0)
#         penalty_factor = np.sqrt(pop) if pop > 0 else 1.0 # é¿å…é™¤0
#         penalized_score = row['mean_score'] / (1 + 0.1 * penalty_factor) # ç¨å¾®æ¸©å’Œä¸€ç‚¹çš„æƒ©ç½š
        
#         final_output.append({
#             'Disease': d_names[d_idx],
#             'Plant': pl_names[pl_idx],
#             'Recommended Pesticide': p_names[p_idx],
#             'Mean Score': row['mean_score'],
#             'Popularity': pop,
#             'Penalized Score': penalized_score,
#             'Std Score': row['std_score'],
#             'Vote Count': f"{int(row['count'])}/{len(SEEDS)}",
#             'Type': 'Known' if is_known else 'Novel Prediction'
#         })

#     df_final = pd.DataFrame(final_output)
#     df_final['Std Score'] = df_final['Std Score'].fillna(0.0)
    
#     # è¿‡æ»¤é€»è¾‘
#     FILTER_KEYWORDS = ["çº¿è™«"] 
#     df_filtered = df_final[~df_final['Disease'].apply(lambda x: any(k in str(x) for k in FILTER_KEYWORDS))]
    
#     df_novel = df_filtered[df_filtered['Type'] == 'Novel Prediction'].copy()
    
#     BROAD_SPECTRUM_PESTICIDES = [
#         "mancozeb", "ä»£æ£®é”°é”Œ", "carbendazim", "å¤šèŒçµ", 
#         "chlorothalonil", "ç™¾èŒæ¸…", "azoxystrobin", "å˜§èŒé…¯"
#     ]
#     # å¤§å°å†™ä¸æ•æ„Ÿè¿‡æ»¤
#     df_novel = df_novel[
#         ~df_novel['Recommended Pesticide'].apply(lambda x: any(k.lower() in str(x).lower() for k in BROAD_SPECTRUM_PESTICIDES))
#     ]
    
#     df_novel = df_novel.sort_values(by=['Mean Score'], ascending=False)
    
#     print(f"\n{'='*120}")
#     print("é›†æˆé¢„æµ‹ç»“æœç¤ºä¾‹ (Top 20):")
#     print(f"{'ç—…å®³':<15} | {'ä½œç‰©':<15} | {'æ¨èå†œè¯':<25} | {'å¾—åˆ†':<6} | {'ç±»å‹'}")
#     print("-" * 120)
#     for _, row in df_novel.head(20).iterrows():
#         print(f"{str(row['Disease'])[:15]:<15} | {str(row['Plant'])[:15]:<15} | {str(row['Recommended Pesticide'])[:25]:<25} | {row['Mean Score']:.4f} | {row['Type']}")
    
#     timestamp = time.strftime('%Y%m%d_%H%M')
#     output_filename = f"Ensemble_Results_{timestamp}.csv"
#     df_novel.to_csv(output_filename, index=False, encoding='utf-8-sig')
#     print(f"\nâœ… ç»“æœå·²ä¿å­˜: {output_filename}")
def aggregate_and_save_results(all_results_dfs, data_full):
    print(f"\n{'='*60}")
    print("ğŸš€ å¼€å§‹èšåˆå¤šç§å­é¢„æµ‹ç»“æœ (å«ä¸¥æ ¼æ–°é¢–æ€§è¿‡æ»¤)")
    print(f"{'='*60}\n")

    if not all_results_dfs:
        print("æ²¡æœ‰æ”¶é›†åˆ°é¢„æµ‹ç»“æœã€‚")
        return

    # 1. æ„å»ºåŸºç¡€æ˜ å°„
    p_names = data_full['pesticide'].names
    d_names = data_full['disease'].names
    pl_names = data_full['plant'].names

    # =========================================================================
    # [æ ¸å¿ƒä¿®æ”¹ 1] æ„å»ºå†œè¯çš„â€œå·²çŸ¥æ²»ç–—ç—…å®³æ¡£æ¡ˆâ€
    # ç›®çš„ï¼šè®°å½•æ¯ä¸ªå†œè¯å·²ç»èƒ½æ²»å“ªäº›ç—…ï¼ˆæ— è®ºåœ¨ä»€ä¹ˆä½œç‰©ä¸Šï¼‰
    # =========================================================================
    print("æ­£åœ¨æ„å»ºå†œè¯å†å²æ²»ç–—æ¡£æ¡ˆ...")
    pe_edges = data_full['pesticide', 'participates_in', 'event'].edge_index.cpu().numpy()
    de_edges = data_full['disease', 'is_target_of', 'event'].edge_index.cpu().numpy()
    
    # Event ID -> Pesticide ID / Disease ID
    e_to_p = dict(zip(pe_edges[1], pe_edges[0]))
    e_to_d = dict(zip(de_edges[1], de_edges[0]))
    
    # è®°å½•æ¯ä¸ªå†œè¯ID å·²çŸ¥çš„ ç—…å®³åç§°é›†åˆ
    # ç»“æ„: {p_idx: {'ç‚­ç–½ç—…', 'ç™½ç²‰ç—…', ...}}
    pesticide_known_diseases = {}
    
    for e_idx in e_to_p:
        if e_idx in e_to_d:
            p_idx = e_to_p[e_idx]
            d_idx = e_to_d[e_idx]
            d_name = d_names[d_idx]
            
            if p_idx not in pesticide_known_diseases:
                pesticide_known_diseases[p_idx] = set()
            pesticide_known_diseases[p_idx].add(d_name)
            
    print("å†å²æ¡£æ¡ˆæ„å»ºå®Œæˆã€‚")

    # 2. åˆå¹¶é¢„æµ‹ç»“æœ
    full_df = pd.concat(all_results_dfs, ignore_index=True)

    # 3. èšåˆ
    agg_df = full_df.groupby(['d_idx', 'pl_idx', 'p_idx']).agg(
        mean_score=('score', 'mean'),
        std_score=('score', 'std'),
        count=('seed', 'count')
    ).reset_index()

    # 4. åŸºç¡€å…±è¯†åº¦è¿‡æ»¤
    MIN_VOTES = max(3, int(len(SEEDS) * 0.8))
    agg_df = agg_df[agg_df['count'] >= MIN_VOTES]

    # 5. æ„å»ºæœ€ç»ˆåˆ—è¡¨ï¼ˆåŠ å…¥ä¸¥æ ¼æ–°é¢–æ€§åˆ¤æ–­ï¼‰
    final_output = []
    
    # å¸¸ç”¨å¹¿è°±å†œè¯åˆ—è¡¨ (å»ºè®®ä¿ç•™è¿‡æ»¤)
    BROAD_SPECTRUM = ["mancozeb", "ä»£æ£®é”°é”Œ", "carbendazim", "å¤šèŒçµ", "chlorothalonil", "ç™¾èŒæ¸…"]

    for _, row in tqdm(agg_df.iterrows(), total=len(agg_df), desc="Filtering"):
        p_idx, d_idx, pl_idx = int(row['p_idx']), int(row['d_idx']), int(row['pl_idx'])
        
        p_name = p_names[p_idx]
        d_name = d_names[d_idx]
        pl_name = pl_names[pl_idx]
        
        # åŸºç¡€è¿‡æ»¤ï¼šå¹¿è°±å†œè¯
        if any(b in str(p_name) for b in BROAD_SPECTRUM):
            continue
            
        # =====================================================================
        # [æ ¸å¿ƒä¿®æ”¹ 2] ä¸¥æ ¼æ–°é¢–æ€§åˆ¤æ–­ (Strict Novelty Check)
        # é€»è¾‘ï¼šå¦‚æœè¿™ä¸ªå†œè¯ä»¥å‰æ²»è¿‡è¿™ç§ç—…ï¼ˆå³ä½¿æ˜¯åœ¨åˆ«çš„ä½œç‰©ä¸Šï¼‰ï¼Œé‚£å°±ä¸æ˜¯æˆ‘ä»¬è¦çš„â€œå…¨æ–°å‘ç°â€
        # =====================================================================
        known_diseases = pesticide_known_diseases.get(p_idx, set())
        
        # å¦‚æœè¯¥ç—…å®³åå­—å‡ºç°åœ¨è¯¥å†œè¯çš„å†å²è®°å½•é‡Œ -> è¯´æ˜æ˜¯â€œè€ç—…æ–°ä½œç‰©â€ (æ‰©ä½œ)
        is_same_disease_extension = d_name in known_diseases
        
        # æˆ‘ä»¬åªä¿ç•™ (æˆ–è€…é«˜äº®) é‚£äº›å†œè¯ä»æœªå¤„ç†è¿‡çš„ç—…å®³
        # è¿™é‡Œæˆ‘å¢åŠ ä¸€ä¸ªæ ‡ç­¾å­—æ®µï¼Œç”±æ‚¨å†³å®šæ˜¯ç›´æ¥è¿‡æ»¤è¿˜æ˜¯åœ¨Excelé‡Œç­›é€‰
        
        # ç­–ç•¥A: ç›´æ¥è¿‡æ»¤æ‰æ‰©ä½œé¢„æµ‹ (åªçœ‹çº¯æ–°çš„)
        # if is_same_disease_extension: continue 
        
        # ç­–ç•¥B: ä¿ç•™ä½†æ ‡è®° (æ¨è)
        prediction_type = "âš ï¸ æ‰©ä½œ (åŒç—…å¼‚ä½œç‰©)" if is_same_disease_extension else "âœ¨ åˆ›æ–° (æœªæ²»è¿‡çš„æ–°ç—…)"
        
        # è®¡ç®—ä¸€äº›è¾…åŠ©åˆ†æ•°
        p_popularity = len(known_diseases) # è¯¥å†œè¯æ²»å¤šå°‘ç§ç—… (ä¸‡é‡‘æ²¹ç¨‹åº¦)
        
        # åªæœ‰å½“å®ƒæ˜¯åˆ›æ–°é¢„æµ‹æ—¶ï¼Œåˆ†æ•°æ‰ä¿æŒåŸæ ·ï¼›å¦‚æœæ˜¯æ‰©ä½œï¼Œå¯ä»¥äººå·¥é™æƒ
        final_score = row['mean_score']
        if is_same_disease_extension:
            final_score *= 0.5 # å¼ºè¡Œé™æƒï¼Œè®©åˆ›æ–°ç»“æœæ’å‰é¢

        final_output.append({
            'Disease': d_name,
            'Plant': pl_name,
            'Recommended Pesticide': p_name,
            'Prediction Type': prediction_type, # æ–°å¢åˆ—
            'Mean Score': row['mean_score'],
            'Adjusted Score': final_score,      # æ–°å¢åˆ—ï¼šé™æƒåçš„åˆ†æ•°
            'Pesticide Breadth': p_popularity,  # è¯¥å†œè¯å·²çŸ¥çš„é˜²æ²»ç—…å®³æ•°é‡
            'Vote Count': f"{int(row['count'])}/{len(SEEDS)}"
        })

    df_final = pd.DataFrame(final_output)
    
    if df_final.empty:
        print("ç­›é€‰åæ— ç»“æœã€‚")
        return

    # 6. æ’åºç­–ç•¥ï¼šä¼˜å…ˆçœ‹â€œåˆ›æ–°â€çš„ï¼Œä¸”åˆ†æ•°é«˜çš„
    # æˆ‘ä»¬æŒ‰ 'Adjusted Score' æ’åºï¼Œè¿™æ ·â€œåˆ›æ–°â€ç±»ä¼šè‡ªç„¶æ’åœ¨å‰é¢
    df_final = df_final.sort_values(by=['Adjusted Score'], ascending=False)
    
    # 7. ä¿å­˜
    timestamp = time.strftime('%Y%m%d_%H%M')
    output_filename = f"Novelty_Prediction_{timestamp}.csv"
    df_final.to_csv(output_filename, index=False, encoding='utf-8-sig')
    
    print(f"\n{'='*120}")
    print("ğŸ”¥ é«˜åˆ›æ–°æ€§é¢„æµ‹ç»“æœç¤ºä¾‹ (Top 20, ä¼˜å…ˆå±•ç¤ºå†œè¯æœªæ²»è¿‡çš„æ–°ç—…):")
    print(f"{'ç±»å‹':<12} | {'ç—…å®³':<10} | {'ä½œç‰©':<10} | {'æ¨èå†œè¯':<20} | {'åŸå¾—åˆ†':<6}")
    print("-" * 120)
    
    for _, row in df_final.head(50).iterrows():
        print(f"{row['Prediction Type']:<12} | {str(row['Disease'])[:10]:<10} | {str(row['Plant'])[:10]:<10} | {str(row['Recommended Pesticide'])[:20]:<20} | {row['Mean Score']:.4f}")
    
    print(f"\nâœ… ç»“æœå·²ä¿å­˜è‡³: {output_filename}")
# ==========================================
# 7. ä¸»æ‰§è¡Œå¾ªç¯
# ==========================================
def run_ensemble_pipeline():
    print(f"Checking data file {DATA_PKL} ...")
    if not os.path.exists(DATA_PKL):
        print(f"é”™è¯¯: æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ {DATA_PKL}")
        return

    # [é‡è¦] ä»…åŠ è½½ä¸€æ¬¡æ•°æ®ï¼Œç„¶åä¼ é€’ç»™å‡½æ•°
    with open(DATA_PKL, "rb") as f:
        data_full = pickle.load(f)
    print("Data loaded successfully.")

    all_run_results = []
    print(f"å¼€å§‹é›†æˆæµç¨‹ï¼Œç§å­åˆ—è¡¨: {SEEDS}")
    
    for seed in SEEDS:
        print(f"\n>>> Processing SEED: {seed}")
        # è®­ç»ƒ
        model_path = train_model(seed, data_full)
        # é¢„æµ‹
        df_seed_result = predict_new_links(seed, model_path, data_full)
        
        if df_seed_result is not None:
            all_run_results.append(df_seed_result)
        
        # å¯é€‰ï¼šåˆ é™¤ä¸´æ—¶æ¨¡å‹èŠ‚çœç©ºé—´
        # if os.path.exists(model_path): os.remove(model_path)
            
    # èšåˆ
    aggregate_and_save_results(all_run_results, data_full)

# if __name__ == "__main__":
#     run_ensemble_pipeline()
import os
import matplotlib.pyplot as plt
from matplotlib import font_manager

def set_manual_font(font_path='SimHei.ttf'):
    """
    æ‰‹åŠ¨åŠ è½½æŒ‡å®šè·¯å¾„çš„å­—ä½“æ–‡ä»¶
    """
    # 1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(font_path):
        print(f"âŒ é”™è¯¯ï¼šåœ¨å½“å‰ç›®å½•ä¸‹æœªæ‰¾åˆ°å­—ä½“æ–‡ä»¶ '{font_path}'")
        print("è¯·ç¡®ä¿ä½ å·²ç»å°† SimHei.ttf ä¸Šä¼ åˆ°äº†è„šæœ¬æ‰€åœ¨çš„ç›®å½•ï¼")
        return

    # 2. å°†å­—ä½“æ–‡ä»¶æ·»åŠ åˆ° Matplotlib çš„å­—ä½“ç®¡ç†å™¨ä¸­
    try:
        # addfont æ˜¯ Matplotlib 3.2+ çš„æ–°ç‰¹æ€§ï¼Œæœ€ç›´æ¥æœ‰æ•ˆ
        font_manager.fontManager.addfont(font_path)
        
        # è·å–è¯¥å­—ä½“çš„å†…éƒ¨åç§°ï¼ˆæœ‰æ—¶å€™æ–‡ä»¶åæ˜¯ SimHei.ttfï¼Œä½†å†…éƒ¨åç§°å« SimHeiï¼‰
        prop = font_manager.FontProperties(fname=font_path)
        font_name = prop.get_name()
        
        # 3. è®¾ç½®å…¨å±€å­—ä½“å‚æ•°
        plt.rcParams['font.sans-serif'] = [font_name] # è®¾ç½®æ— è¡¬çº¿å­—ä½“ä¸ºè¯¥å­—ä½“
        plt.rcParams['axes.unicode_minus'] = False    # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
        
        print(f"âœ… å­—ä½“åŠ è½½æˆåŠŸï¼å·²ä½¿ç”¨: {font_name} ({font_path})")
        
    except Exception as e:
        print(f"âŒ å­—ä½“åŠ è½½å‡ºé”™: {e}")

# ==========================================
# æ‰§è¡Œé…ç½®
# ==========================================
# å‡è®¾ä½ ä¸Šä¼ çš„æ–‡ä»¶åæ˜¯ SimHei.ttf
set_manual_font('SimHei.ttf')
# ==========================================
# åœ¨è„šæœ¬æœ€å¼€å§‹è¿è¡Œä¸€æ¬¡å³å¯
# ==========================================
# configure_chinese_font()
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
import torch
import torch.nn.functional as F
import os

# è®¾ç½®ç»˜å›¾é£æ ¼
sns.set(style="whitegrid", context="talk")
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS'] # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾
plt.rcParams['axes.unicode_minus'] = False # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·

def analyze_prototype_mechanisms(model, data, node_type, device, output_dir="vis_results"):
    """
    é’ˆå¯¹åŸå‹æ¨¡å—è¿›è¡Œæ·±åº¦åˆ†æï¼šçƒ­åŠ›å›¾ã€åˆ†å¸ƒå›¾ã€è¯­ä¹‰è¡¨
    """
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    print(f"\nğŸ”¬ æ­£åœ¨æ·±åº¦åˆ†æ [{node_type}] çš„åŸå‹æœºåˆ¶...")
    
    # 1. è·å–å¯¹åº”çš„ Refiner å’Œåç§°
    if node_type == 'pesticide':
        refiner = model.refiner_p
        names = data['pesticide'].names
    elif node_type == 'disease':
        refiner = model.refiner_d
        names = data['disease'].names
    elif node_type == 'plant':
        refiner = model.refiner_pl
        names = data['plant'].names
    else:
        return

    # 2. æå–æ•°æ® (å‰å‘ä¼ æ’­ä¸€æ¬¡ä»¥è·å–æœ€æ–°çš„èŠ‚ç‚¹åµŒå…¥)
    model.eval()
    with torch.no_grad():
        # è·å–åŸå‹å‘é‡ (K, Dim)
        prototypes = refiner.prototypes.data.cpu()
        # è·å–ç»è¿‡ç½‘ç»œå¤„ç†åçš„èŠ‚ç‚¹å‘é‡ (N, Dim)
        out = model(data.x_dict, data.edge_index_dict)
        node_embs = out[node_type].cpu()

    # å½’ä¸€åŒ–ï¼Œæ–¹ä¾¿è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    prototypes_norm = F.normalize(prototypes, dim=1)
    node_embs_norm = F.normalize(node_embs, dim=1)
    
    num_protos = prototypes.shape[0]

    # ==========================================
    # å¯è§†åŒ– A: åŸå‹è‡ªç›¸ä¼¼åº¦çƒ­åŠ›å›¾ (Diversity Check)
    # ==========================================
    # è®¡ç®—åŸå‹ä¹‹é—´çš„ç›¸ä¼¼åº¦çŸ©é˜µ (K, K)
    proto_sim_matrix = torch.mm(prototypes_norm, prototypes_norm.t()).numpy()
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(proto_sim_matrix, annot=True, fmt=".2f", cmap="coolwarm", 
                vmin=-0.2, vmax=1.0, square=True,
                xticklabels=[f"P{i}" for i in range(num_protos)],
                yticklabels=[f"P{i}" for i in range(num_protos)])
    plt.title(f"{node_type} - Prototype Similarity (Diversity Check)")
    plt.tight_layout()
    save_path = os.path.join(output_dir, f"{node_type}_similarity_heatmap.png")
    plt.savefig(save_path, dpi=300)
    plt.close()
    print(f"  [1/3] è‡ªç›¸ä¼¼çƒ­åŠ›å›¾å·²ä¿å­˜: {save_path}")

    # ==========================================
    # å¯è§†åŒ– B: èŠ‚ç‚¹å½’å±åˆ†å¸ƒå›¾ (Utilization Check)
    # ==========================================
    # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹æœ€æ¥è¿‘å“ªä¸ªåŸå‹
    # (N, K)
    similarity_scores = torch.mm(node_embs_norm, prototypes_norm.t())
    # è·å–æ¯ä¸ªèŠ‚ç‚¹å½’å±çš„åŸå‹ ID
    assignments = torch.argmax(similarity_scores, dim=1).numpy()
    
    # ç»Ÿè®¡æ¯ä¸ªåŸå‹æœ‰å¤šå°‘ä¸ªèŠ‚ç‚¹
    counts = pd.Series(assignments).value_counts().sort_index()
    # è¡¥å…¨å¯èƒ½ä¸º0çš„åŸå‹
    for i in range(num_protos):
        if i not in counts:
            counts[i] = 0
            
    plt.figure(figsize=(12, 6))
    ax = sns.barplot(x=counts.index, y=counts.values, palette="viridis")
    ax.set_xlabel("Prototype ID")
    ax.set_ylabel("Number of Assigned Nodes")
    ax.set_title(f"{node_type} - Node Assignment Distribution")
    # åœ¨æŸ±å­ä¸Šæ ‡æ•°å€¼
    for i, v in enumerate(counts.values):
        ax.text(i, v + max(counts.values)*0.01, str(v), ha='center')
    
    plt.tight_layout()
    save_path = os.path.join(output_dir, f"{node_type}_assignment_dist.png")
    plt.savefig(save_path, dpi=300)
    plt.close()
    print(f"  [2/3] å½’å±åˆ†å¸ƒå›¾å·²ä¿å­˜: {save_path}")

    # ==========================================
    # åˆ†æ C: åŸå‹è¯­ä¹‰è§£ç è¡¨ (Interpretation)
    # ==========================================
    # æ‰¾å‡ºæ¯ä¸ªåŸå‹ç›¸ä¼¼åº¦æœ€é«˜çš„ Top-5 èŠ‚ç‚¹
    print(f"  [3/3] ç”Ÿæˆè¯­ä¹‰è§£é‡Šè¡¨...")
    
    proto_semantics = []
    
    # ä½¿ç”¨ similarity_scores (N, K) -> è½¬ç½®ä¸º (K, N)
    sim_t = similarity_scores.t()
    
    for i in range(num_protos):
        # æ‰¾åˆ°è¯¥åŸå‹å¾—åˆ†æœ€é«˜çš„ Top K ç´¢å¼•
        values, indices = torch.topk(sim_t[i], k=8)
        
        # è·å–åç§°
        top_names = [str(names[idx.item()]) for idx in indices]
        
        # è®°å½•æ•°æ®
        proto_semantics.append({
            "Prototype ID": f"P{i}",
            "Count": counts[i],
            "Representative Entities": ", ".join(top_names[:5]), # åªå±•ç¤ºå‰5ä¸ªé˜²æ­¢å¤ªé•¿
            "Top 1 Score": f"{values[0].item():.4f}" # è®°å½•æœ€ç›¸ä¼¼çš„é‚£ä¸ªåˆ†æ•°ï¼Œçœ‹ç¡®ä¿¡åº¦
        })
        
    df_semantics = pd.DataFrame(proto_semantics)
    
    # æ‰“å°è¡¨æ ¼
    print(f"\n{'-'*80}")
    print(f"è¯­ä¹‰è§£é‡Š: {node_type}")
    print(f"{'-'*80}")
    # è®¾ç½®pandasæ˜¾ç¤ºé€‰é¡¹ä»¥ä¾¿åœ¨ç»ˆç«¯çœ‹å…¨
    pd.set_option('display.max_colwidth', 100) 
    print(df_semantics[["Prototype ID", "Count", "Representative Entities"]])
    print(f"{'-'*80}\n")
    
    # ä¿å­˜ CSV
    csv_path = os.path.join(output_dir, f"{node_type}_prototype_semantics.csv")
    df_semantics.to_csv(csv_path, index=False, encoding='utf-8-sig')


def run_prototype_analysis_pipeline(seed, model_path):
    with open(DATA_PKL, "rb") as f:
        data_full = pickle.load(f)
    num_pesticides = data_full['pesticide'].num_nodes
    num_events = data_full['event'].num_nodes
    print(f"\n{'='*60}")
    print(f"ğŸš€ å¼€å§‹åŸå‹æ·±åº¦åˆ†æ (Seed: {seed})")
    print(f"ğŸ“‚ ç»“æœå°†ä¿å­˜è‡³: ./vis_analysis_results/")
    print(f"{'='*60}")
    
    # 1. æ¨¡å‹åˆå§‹åŒ– (ç»“æ„å¿…é¡»ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´)
    input_channels = {}
    for node_type in ['pesticide', 'disease', 'plant']:
        input_channels[node_type] = data_full[node_type].x.shape[1]
    PROTO_CONFIG = {
    'pesticide': 10,  # å†œè¯ç§ç±»ç¹å¤šï¼Œæœºåˆ¶å¤æ‚ï¼Œç»™å¤šä¸€ç‚¹
    'disease': 10,    # ç—…å®³ç§ç±»ä¸­ç­‰
    'plant': 10       # ä½œç‰©ç§ç±»ç›¸å¯¹è¾ƒå°‘ï¼Œæˆ–è€…æˆ‘ä»¬åªå…³æ³¨å¤§ç±»ï¼Œç»™å°‘ä¸€ç‚¹
    }
    model = MultiModelNetV2(
        metadata=data_full.metadata(),
        input_channels_dict=input_channels,
        num_events=num_events,
        embed_dim=EMBED_DIM,
        heads=NUM_HEADS,
        proto_config=PROTO_CONFIG 
    ).to(DEVICE)
    
    # 2. åŠ è½½æƒé‡
    if not os.path.exists(model_path):
        print(f"âŒ é”™è¯¯ï¼šæ¨¡å‹æ–‡ä»¶ {model_path} ä¸å­˜åœ¨ã€‚è¯·å…ˆè®­ç»ƒã€‚")
        return
        
    model.load_state_dict(torch.load(model_path, map_location=DEVICE))
    print("âœ… æ¨¡å‹æƒé‡å·²åŠ è½½ã€‚")

    # 3. å‡†å¤‡æ•°æ®
    data_vis = data_full.to(DEVICE)
    
    # 4. æ‰§è¡Œä¸‰å¤§ç±»èŠ‚ç‚¹çš„åˆ†æ
    analyze_prototype_mechanisms(model, data_vis, 'disease', DEVICE, output_dir="vis_analysis_results")
    analyze_prototype_mechanisms(model, data_vis, 'plant', DEVICE, output_dir="vis_analysis_results")
    analyze_prototype_mechanisms(model, data_vis, 'pesticide', DEVICE, output_dir="vis_analysis_results")
    
    print("\nğŸ‰ åˆ†æå…¨éƒ¨å®Œæˆï¼è¯·æŸ¥çœ‹ ./vis_analysis_results/ æ–‡ä»¶å¤¹ä¸‹çš„å›¾ç‰‡å’ŒCSVæ–‡ä»¶ã€‚")

if __name__ == "__main__":
    # ä½¿ç”¨é›†æˆå­¦ä¹ ä¸­çš„ç¬¬ä¸€ä¸ªç§å­åŠå…¶å¯¹åº”çš„æ¨¡å‹
    target_seed = SEEDS[3] 
    target_model_path = f"/home/fine-tune/gnn/best_model_final.pth"
    
    # ä¸ºäº†æ¼”ç¤ºï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½ å¯ä»¥é€‰æ‹©å…ˆä¸è·‘ï¼Œæˆ–è€…åœ¨è¿™é‡Œè‡ªåŠ¨è§¦å‘ä¸€æ¬¡å¿«é€Ÿè®­ç»ƒ
    if os.path.exists(target_model_path):
        run_prototype_analysis_pipeline(target_seed, target_model_path)
    else:
        print(f"è¯·ç¡®ä¿ {target_model_path} å­˜åœ¨ (å¯ä»¥é€šè¿‡è¿è¡Œä¸»è®­ç»ƒè„šæœ¬ç”Ÿæˆ)")

# -*- coding: utf-8 -*-
import os
import pickle
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np
from torch_geometric.nn import HGTConv

# ==========================================
# 1. æ··åˆå­—ä½“é…ç½® (æ”¯æŒ SCI é£æ ¼ + ä¸­æ–‡)
# ==========================================
# ä¼˜å…ˆä½¿ç”¨ Times New Roman (è‹±æ–‡)ï¼Œå¦‚æœç¼ºå­—åˆ™å›é€€åˆ° SimHei (ä¸­æ–‡)
# æ³¨æ„ï¼šMatplotlib çš„ font.serif åˆ—è¡¨æœºåˆ¶å¯ä»¥å®ç°æ··åˆæ˜¾ç¤º
plt.rcParams['font.family'] = 'serif'
plt.rcParams['font.serif'] = ['Times New Roman', 'SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['font.size'] = 12
plt.rcParams['axes.linewidth'] = 1.0

# é…ç½®è·¯å¾„
DATA_PKL = "data_reified.pkl"
MODEL_PATH = "/home/fine-tune/gnn/best_model_final.pth" 
OUTPUT_DIR = "sci_plots_output_mixed"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

# ==========================================
# 2. ä¸­è‹±å¯¹ç…§å­—å…¸ (ä»…ä¿ç•™ä½œç‰©å’Œç—…å®³)
# ==========================================
TRANSLATION_MAP = {
    # --- Sample Targets ---
    "æ°´ç¨»": "Rice",
    "è‹¹æœ": "Apple",
    "ç•ªèŒ„": "Tomato",
    "æ£‰èŠ±": "Cotton",
    "ç‚­ç–½ç—…": "Anthracnose",
    "ç™½ç²‰ç—…": "Powdery Mildew",
    "çº¹æ¯ç—…": "Sheath Blight",
    "æ ¹ç»“çº¿è™«": "Root-knot Nematode",
    "çŒ•çŒ´æ¡ƒ":"Kiwifruit",

    # --- Block 1: ç—…å®³ (Disease) ---
    "ç‚­ç–½ç—…": "Anthracnose", "ç™½ç²‰ç—…": "Powdery Mildew",
    "å¶é”ˆç—…": "Leaf Rust", "å‚¨è—ç—…å®³": "Storage Disease",
    "èŒæ ¸ç—…": "Sclerotinia Rot", "èƒ¡éº»å¶æ–‘ç—…": "Brown Spot", 
    "ç™½é”ˆç—…": "White Rust", "è‹—ç‚­ç–½ç—…": "Seedling Anthracnose",
    "é”ˆå£è™±": "Rust Mite", "ç¾å›½ç™½è›¾": "Fall Webworm",
    "ç™½ç»¢ç—…": "Southern Blight", "å…¨èš€ç—…": "Take-all",
    "æ¯èç—…": "Fusarium Wilt", "ç«‹æ¯ç—…": "Damping-off",
    "è’‚è…ç—…": "Stem End Rot", "ç»¿éœ‰ç—…": "Green Mold",
    "èœé’è™«": "Cabbage Caterpillar", "æ¾æ¯›è™«": "Pine Caterpillar",
    "å¶éœ‰ç—…": "Leaf Mold", "èµ¤éœ‰ç—…": "Fusarium Head Blight",

    # --- Block 2: ä½œç‰© (Plant) ---
    "éƒé‡‘é¦™": "Tulip", "æ°´ç¨»ç§»æ ½ç”°": "Transplanted Rice Field",
    "æ¸æ": "Wolfberry", "èœè±†": "Common Bean",
    "å¤§è‘±": "Welsh Onion", "èŠèŠ±": "Chrysanthemum",
    "ç”˜è”—": "Sugarcane", "ç»¿è": "Duckweed",
    "æŸ‘æ©˜": "Citrus", "è‹¹æœæ ‘": "Apple Tree",
    "èŠ±å‰": "Flowers", "è§‚èµç™¾åˆ": "Ornamental Lily",
    "èéº¦": "Buckwheat", "é’æ¢…": "Green Plum",
    "è§‚èµèŠèŠ±": "Ornamental Chrysanthemum", "è‘¡è„": "Grape",
    "æ‡æ·æ ‘": "Loquat Tree", "æ¨æ¢…": "Chinese Bayberry",
    "äººå‚": "Ginseng", "è¾£æ¤’": "Chili Pepper",
}

def translate(text):
    """ç¿»è¯‘å‡½æ•°"""
    if text in TRANSLATION_MAP:
        return TRANSLATION_MAP[text]
    for k, v in TRANSLATION_MAP.items():
        if k == text: return v
    return text 

# ==========================================
# 3. æ¨¡å‹å®šä¹‰ (ä¿æŒä¸å˜)
# ==========================================
class AttentionFusion(nn.Module):
    def __init__(self, embed_dim=128, num_heads=4):
        super().__init__()
        self.attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)
        self.layer_norm = nn.LayerNorm(embed_dim)
    def forward(self, inputs):
        stacked = torch.stack(inputs, dim=1) 
        attn_output, _ = self.attention(stacked, stacked, stacked)
        return self.layer_norm(attn_output.mean(dim=1))

class HGTBlock(nn.Module):
    def __init__(self, in_channels, out_channels, metadata, heads=4, dropout=0.2):
        super().__init__()
        self.hgt = HGTConv(in_channels, out_channels, metadata, heads)
        self.norm = nn.LayerNorm(out_channels)
        self.dropout = nn.Dropout(dropout)
    def forward(self, x_dict, edge_dict, res_dict=None):
        x_new = self.hgt(x_dict, edge_dict)
        x_out = {}
        for k in x_new:
            x = x_new[k]
            if res_dict is not None:
                res = res_dict.get(k, None)
                if res is not None and res.shape[-1] == x.shape[-1]:
                    x = x + res
            x_out[k] = self.dropout(F.relu(self.norm(x)))
        return x_out

class PrototypeRefiner(nn.Module):
    def __init__(self, num_prototypes, embed_dim, k=3, temperature=0.1):
        super().__init__()
        self.prototypes = nn.Parameter(torch.empty(num_prototypes, embed_dim))
        nn.init.orthogonal_(self.prototypes)
        self.proto_transform = nn.Sequential(nn.Linear(embed_dim, embed_dim), nn.ReLU(), nn.LayerNorm(embed_dim))
        self.gate_net = nn.Sequential(nn.Linear(embed_dim * 2, embed_dim), nn.Sigmoid())
        self.final_norm = nn.LayerNorm(embed_dim)
        self.k = min(k, num_prototypes)
        self.warmup = True 
        self.temperature = temperature
    def set_warmup(self, status: bool): self.warmup = status
    def get_regularization_loss(self, batch_mean_probs): return 0.0
    def forward(self, x_instance): return None, None, None

class MultiModelNetV2(nn.Module):
    def __init__(self, metadata, input_channels_dict, num_events, embed_dim=128, heads=4, 
                 proto_config={'pesticide': 5, 'disease': 5, 'plant': 5}):
        super().__init__()
        self.input_projs = nn.ModuleDict()
        for node_type, in_dim in input_channels_dict.items():
            if node_type != 'event': self.input_projs[node_type] = nn.Linear(in_dim, embed_dim)
        self.event_emb = nn.Embedding(num_events, embed_dim)
        self.block1 = HGTBlock(embed_dim, embed_dim, metadata, heads)
        self.block2 = HGTBlock(embed_dim, embed_dim, metadata, heads)
        self.block3 = HGTBlock(embed_dim, embed_dim, metadata, heads) 
        self.fusion_p = AttentionFusion(embed_dim, heads)
        self.fusion_d = AttentionFusion(embed_dim, heads)
        self.fusion_pl = AttentionFusion(embed_dim, heads)
        num_p, num_d, num_pl = proto_config.get('pesticide', 10), proto_config.get('disease', 10), proto_config.get('plant', 10)
        self.refiner_p = PrototypeRefiner(num_p, embed_dim, k=min(3, num_p))
        self.refiner_d = PrototypeRefiner(num_d, embed_dim, k=min(3, num_d))
        self.refiner_pl = PrototypeRefiner(num_pl, embed_dim, k=min(3, num_pl))

    def forward(self, x_dict, edge_index_dict):
        x_emb = {}
        for ntype, x in x_dict.items():
            if ntype == 'event': x_emb[ntype] = self.event_emb(torch.arange(x.shape[0], device=x.device))
            elif ntype in self.input_projs: x_emb[ntype] = F.relu(self.input_projs[ntype](x))
        x1 = self.block1(x_emb, edge_index_dict, x_emb)
        x2 = self.block2(x1, edge_index_dict, x1)
        x3 = self.block3(x2, edge_index_dict, x2)
        p_raw = self.fusion_p([x1['pesticide'], x2['pesticide'], x3['pesticide']])
        d_raw = self.fusion_d([x1['disease'], x2['disease'], x3['disease']])
        pl_raw = self.fusion_pl([x1['plant'], x2['plant'], x3['plant']])
        return {'pesticide': p_raw, 'disease': d_raw, 'plant': pl_raw}

# ==========================================
# 4. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° (å·²ä¿®æ”¹ï¼šå†œè¯ä¸ç¿»è¯‘)
# ==========================================
def get_prototype_keywords_sci(model, data, node_type, top_k=2):
    """
    è®¡ç®—åŸå‹å…³é”®è¯ã€‚
    - å¦‚æœæ˜¯ 'pesticide'ï¼Œä¿ç•™åŸå§‹ä¸­æ–‡ã€‚
    - å¦åˆ™ï¼Œç¿»è¯‘ä¸ºè‹±æ–‡ã€‚
    """
    if node_type == 'pesticide':
        refiner = model.refiner_p
        names = data['pesticide'].names
    elif node_type == 'disease':
        refiner = model.refiner_d
        names = data['disease'].names
    elif node_type == 'plant':
        refiner = model.refiner_pl
        names = data['plant'].names
    
    model.eval()
    with torch.no_grad():
        out = model(data.x_dict, data.edge_index_dict)
        entity_feats = F.normalize(out[node_type].cpu(), dim=1)
        prototypes = F.normalize(refiner.prototypes.data.cpu(), dim=1)
    
    scores = torch.mm(entity_feats, prototypes.t()) / refiner.temperature
    scores_t = scores.t()
    
    proto_labels = []
    for i in range(prototypes.shape[0]):
        _, indices = torch.topk(scores_t[i], k=top_k)
        
        keywords = []
        for idx in indices:
            raw_name = str(names[idx.item()])
            # [å…³é”®é€»è¾‘] å†œè¯ä¸ç¿»è¯‘ï¼Œå…¶ä»–ç¿»è¯‘
            if node_type == 'pesticide':
                keywords.append(raw_name)
            else:
                keywords.append(translate(raw_name))
        
        short_keywords = "\n".join(keywords)
        proto_labels.append(f"P{i}\n({short_keywords})")
        
    return proto_labels

def plot_sample_similarity_sci(model, data, node_type, sample_names):
    """ç”Ÿæˆ SCI é£æ ¼å›¾è¡¨ (å†œè¯æ˜¾ç¤ºä¸­æ–‡ï¼Œå…¶ä»–æ˜¾ç¤ºè‹±æ–‡)"""
    print(f"\nğŸ¨ Generating plots for [{node_type}] (No translation for pesticides)...")
    
    if node_type == 'pesticide':
        names_list = data['pesticide'].names
        refiner = model.refiner_p
    elif node_type == 'disease':
        names_list = data['disease'].names
        refiner = model.refiner_d
    elif node_type == 'plant':
        names_list = data['plant'].names
        refiner = model.refiner_pl
    
    name_to_idx = {name: i for i, name in enumerate(names_list)}
    
    # è·å–æ ‡ç­¾ (å†œè¯å·²å¤„ç†ä¸ºä¸ç¿»è¯‘)
    proto_labels = get_prototype_keywords_sci(model, data, node_type, top_k=2)
    num_protos = len(proto_labels)
    
    model.eval()
    with torch.no_grad():
        out = model(data.x_dict, data.edge_index_dict)
        all_node_embs = out[node_type].cpu()
        prototypes = refiner.prototypes.data.cpu()
        
    all_node_embs_norm = F.normalize(all_node_embs, dim=1)
    prototypes_norm = F.normalize(prototypes, dim=1)
    
    num_samples = len(sample_names)
    cols = 2
    rows = math.ceil(num_samples / cols)
    
    fig, axes = plt.subplots(rows, cols, figsize=(12, 4.5 * rows), constrained_layout=True)
    if num_samples == 1: axes = [axes]
    axes = np.array(axes).flatten()
    
    valid_count = 0
    
    for i, sample_name_cn in enumerate(sample_names):
        idx = -1
        # æŸ¥æ‰¾é€»è¾‘
        if sample_name_cn in name_to_idx:
            idx = name_to_idx[sample_name_cn]
        else:
            candidates = [k for k in name_to_idx.keys() if sample_name_cn in str(k)]
            if candidates:
                print(f"  > '{sample_name_cn}' matched to '{candidates[0]}'")
                sample_name_cn = candidates[0]
                idx = name_to_idx[sample_name_cn]
        
        if idx == -1:
            print(f"âš ï¸ Warning: Sample '{sample_name_cn}' not found.")
            continue
            
        valid_count += 1
        
        # [å…³é”®é€»è¾‘] æ ‡é¢˜æ˜¾ç¤ºï¼šå†œè¯ç”¨åŸåï¼Œå…¶ä»–ç”¨è‹±æ–‡
        if node_type == 'pesticide':
            display_title = sample_name_cn
        else:
            display_title = translate(sample_name_cn)
        
        sample_vec = all_node_embs_norm[idx].unsqueeze(0)
        sim_scores = torch.mm(sample_vec, prototypes_norm.t()).squeeze().numpy()
        
        ax = axes[i]
        norm = plt.Normalize(-0.5, 1.0)
        colors = plt.cm.coolwarm(norm(sim_scores))
        
        bars = ax.bar(range(num_protos), sim_scores, color=colors, 
                      edgecolor='black', linewidth=0.6, width=0.7)
        
        # æ ‡é¢˜
        ax.set_title(f"Sample: {display_title}", fontsize=14, fontweight='bold', loc='left')
        
        # åæ ‡è½´
        ax.set_xticks(range(num_protos))
        ax.set_xticklabels(proto_labels, rotation=45, ha='right', fontsize=10)
        ax.set_ylabel("Cosine Similarity", fontsize=11)
        ax.set_ylim(-0.4, 1.1)
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        ax.axhline(0, color='black', linestyle='-', linewidth=0.8)
        
        for bar in bars:
            height = bar.get_height()
            offset = 0.05 if height >= 0 else -0.12
            ax.text(bar.get_x() + bar.get_width()/2., height + offset,
                    f'{height:.2f}',
                    ha='center', va='bottom', fontsize=9, color='black')

    for j in range(valid_count, len(axes)):
        axes[j].axis('off')
        
    save_base = f"{OUTPUT_DIR}/sci_analysis_{node_type}"
    plt.savefig(f"{save_base}.pdf", format='pdf', bbox_inches='tight')
    plt.savefig(f"{save_base}.png", dpi=300, bbox_inches='tight')
    print(f"âœ… Saved to: {save_base}.png")

# ==========================================
# 5. ä¸»ç¨‹åº
# ==========================================
def main():
    if not os.path.exists(DATA_PKL):
        print(f"âŒ Data file {DATA_PKL} not found.")
        return
        
    print("Loading data...")
    with open(DATA_PKL, "rb") as f:
        data_full = pickle.load(f)
    
    num_events = data_full['event'].num_nodes
    input_channels = {k: data_full[k].x.shape[1] for k in data_full.node_types}

    PROTO_CONFIG = {'pesticide': 10, 'disease': 10, 'plant': 10}
    
    model = MultiModelNetV2(
        metadata=data_full.metadata(),
        input_channels_dict=input_channels,
        num_events=num_events,
        embed_dim=128,
        heads=4,
        proto_config=PROTO_CONFIG 
    ).to(DEVICE)
    
    if os.path.exists(MODEL_PATH):
        print(f"Loading weights from {MODEL_PATH}")
        model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE), strict=False)
    else:
        print(f"âŒ Model file {MODEL_PATH} not found.")
        return

    data_vis = data_full.to(DEVICE)

    # -----------------------------------------------------------
    # 1. ä½œç‰© (ç¿»è¯‘æˆè‹±æ–‡)
    # -----------------------------------------------------------
    plant_targets = ["çŒ•çŒ´æ¡ƒ", "è‹¹æœ", "ç‰ç±³", "çƒŸè‰"] 
    plot_sample_similarity_sci(model, data_vis, 'plant', plant_targets)

    # -----------------------------------------------------------
    # 2. ç—…å®³ (ç¿»è¯‘æˆè‹±æ–‡)
    # -----------------------------------------------------------
    disease_targets = ["è¤æ–‘ç—…", "æ–‘ç‚¹è½å¶ç—…", "ä¸é»‘ç©—ç—…", "é‡ç«ç—…"]
    plot_sample_similarity_sci(model, data_vis, 'disease', disease_targets)

    # -----------------------------------------------------------
    # 3. å†œè¯ (ä¸ç¿»è¯‘ï¼Œä¿æŒä¸­æ–‡ï¼Œå­—ä½“å›é€€åˆ° SimHei)
    # -----------------------------------------------------------
    # pesticide_targets = ["ä»£æ£®é”°é”Œ", "é˜¿ç»´èŒç´ ", "å¡è™«å•‰", "æˆŠå”‘é†‡"]
    pesticide_samples = ["tebuconazole", "thiophanate-methyl", "hexaconazole",'copper oxychloride']
    plot_sample_similarity_sci(model, data_vis, 'pesticide', pesticide_samples)

if __name__ == "__main__":
    main()